References:
[1] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce.
Chip-chat: Challenges and opportunities in conversational hardware
design. In 2023 ACM/IEEE 5th Workshop on Machine Learning for
CAD (MLCAD), pages 1â€“6. IEEE, 2023.

[2] Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng,
Haisheng Zheng, and Bei Yu. Chateda: A large language model powered
autonomous agent for eda, 2024.

[3] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge
in a neural network, 2015.

[4] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank
adaptation of large language models. arXiv preprint arXiv:2106.09685,
2021.

[5] Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel
Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra
Banerjee, Ismet Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro,
Arjun Chaudhuri, Sharon Clay, Bill Dally, Laura Dang, Parikshit Desh-
pande, Siddhanth Dhodhi, Sameer Halepete, Eric Hill, Jiashang Hu,
Sumit Jain, Ankit Jindal, Brucek Khailany, George Kokai, Kishor Kunal,
Xiaowei Li, Charley Lind, Hao Liu, Stuart Oberman, Sujeet Omar,
Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang Shao,
Hanfei Sun, Pratik P Suthar, Varun Tej, Walker Turner, Kaizhe Xu, and
Haoxing Ren. Chipnemo: Domain-adapted llms for chip design, 2024.

[6] Nirjhor Rouf, Fin Amin, Paul D. Franzon. "Can Low Rank Knowledge 
Distillation be Useful for Microelectronics Reasoning?", LAD, 2024

[7] Song Han, Jeff Pool, John Tran, William J. Dally. "Learning both Weights 
and Connections for Efficient Neural Networks", Neurips, 2015

[8] Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev. "COMPRESSING DEEP 
CONVOLUTIONAL NETWORKS USING VECTOR QUANTIZATION", arxiv, 2014

[9] Jason Wei, Zuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, 
Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou. "Chain-of-Thought Prompting 
Elicits Reasoning in Large Language Models", Neurips, 2022.

[10] Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu. "Teaching 
Language Models to Self-Improve through Interactive Demonstrations", arxiv, 2023

[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,
Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,
Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,
Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba.
"Evaluating Large Language Models Trained on Code", arxiv, 2021


